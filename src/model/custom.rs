use std::{collections::HashMap, ffi::CStr, sync::Arc};

use pgrx::{
    pg_sys::{panic::ErrorReportable, AsPgCStr},
    prelude::PgHeapTuple,
    WhoAllocated,
};
use serde::{Deserialize, Serialize};
use validator::{Validate, ValidationError};

use crate::{text_analyzer::get_text_analyzer, utils::spi_get_one};

use super::{validate_new_model_name, ModelConfig, TokenizerModel, MODEL_OBJECT_POOL};

#[derive(Debug)]
pub struct CustomModel {
    name: String,
}

impl CustomModel {
    pub fn new(name: &str, _config: &CustomModelConfig) -> Self {
        CustomModel {
            name: name.to_string(),
        }
    }
}

impl TokenizerModel for CustomModel {
    fn apply(&self, text: String) -> Vec<u32> {
        let query = format!(
            r#"SELECT id FROM tokenizer_catalog."model_{}" WHERE token = $1"#,
            self.name
        );

        let id = pgrx::Spi::connect(|client| {
            let tuptable = client
                .select(&query, None, &[text.into()])
                .unwrap_or_report();
            match tuptable.first().get_one::<i32>() {
                Ok(id) => Some(id.unwrap()),
                Err(e) if matches!(e, pgrx::spi::SpiError::NoTupleTable) => None,
                Err(e) => panic!("Error fetching TextAnalyzer: {}", e),
            }
        });

        if let Some(id) = id {
            vec![u32::try_from(id).unwrap()]
        } else {
            vec![]
        }
    }

    fn apply_batch(&self, tokens: Vec<String>) -> Vec<u32> {
        let query = format!(
            r#"SELECT id, token FROM tokenizer_catalog."model_{}" WHERE token = ANY($1)"#,
            self.name
        );

        let mut token_map = HashMap::new();
        pgrx::Spi::connect(|client| {
            let tuptable = client
                .select(&query, None, &[tokens.clone().into()])
                .unwrap_or_report();
            for tup in tuptable {
                let id: i32 = tup.get(1).unwrap_or_report().expect("no id value");
                let id = u32::try_from(id).expect("id is not a valid u32");
                let token: String = tup.get(2).unwrap_or_report().expect("no token value");
                token_map.insert(token, id);
            }
        });

        tokens
            .into_iter()
            .filter_map(|token| token_map.get(&token).copied())
            .collect()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, Validate)]
#[serde(deny_unknown_fields)]
#[validate(schema(function = "CustomModelConfig::validate_column_name"))]
pub struct CustomModelConfig {
    table: String,
    column: String,
    text_analyzer: String,
}

impl CustomModelConfig {
    fn validate_column_name(&self) -> Result<(), ValidationError> {
        if self.column.contains("$col$") {
            return Err(ValidationError::new("column name cannot contain '$col$'"));
        }
        Ok(())
    }
}

// 1. validate the model name and config
// 2. validate if text analyzer exists
// 3. create word table && create trigger && insert existing tokens
// 4. insert model into model table
// 5. insert model into model object pool
// do 3 4 5 in a transaction, if any step fails, rollback
#[pgrx::pg_extern(volatile, parallel_safe)]
fn create_custom_model(name: &str, config: &str) {
    validate_new_model_name(&name).unwrap();

    let config: CustomModelConfig = toml::from_str(config).unwrap();
    let table = quote_identifier(&config.table);
    let column = quote_identifier(&config.column);

    get_text_analyzer(&config.text_analyzer);

    let create_word_table = format!(
        r#"
        CREATE TABLE tokenizer_catalog."model_{}" (
            id int GENERATED BY DEFAULT AS IDENTITY,
            token TEXT NOT NULL UNIQUE
        );
        "#,
        name
    );
    let create_trigger = format!(
        r#"
        CREATE TRIGGER "model_{}_trigger"
        BEFORE INSERT OR UPDATE OF {}
        ON {}
        FOR EACH ROW
        EXECUTE FUNCTION tokenizer_catalog.custom_model_insert_trigger('{}', $col${}$col$, {});
        "#,
        name, column, table, name, config.column, config.text_analyzer
    );
    let insert_existing_tokens = format!(
        r#"
        WITH tokens AS (
            SELECT unnest(tokenizer_catalog.apply_text_analyzer_for_custom_model({}, '{}')) AS token
            FROM {}
        )
        INSERT INTO tokenizer_catalog."model_{}" (token)
        SELECT DISTINCT token
        FROM tokens
        ON CONFLICT (token) DO NOTHING;
        "#,
        column, config.text_analyzer, table, name
    );
    let insert_model = r#"
        INSERT INTO tokenizer_catalog.model (name, config) VALUES ($1, $2)
        ON CONFLICT (name) DO NOTHING RETURNING 1
        "#;
    let custom_model = CustomModel::new(name, &config);
    let config_str = serde_json::to_string(&ModelConfig::Custom(config)).unwrap();

    pgrx::Spi::connect_mut(|client| {
        client.update(&create_word_table, None, &[]).unwrap();
        client.update(&create_trigger, None, &[]).unwrap();
        client.update(&insert_existing_tokens, None, &[]).unwrap();

        let tuptable = client
            .update(insert_model, None, &[name.into(), config_str.into()])
            .unwrap();
        if tuptable.len() == 0 {
            panic!("Model already exists: {}", name);
        }

        if MODEL_OBJECT_POOL
            .insert(name.to_string(), Arc::new(custom_model))
            .is_some()
        {
            panic!("Model already exists: {}", name);
        }
    });
}

#[pgrx::pg_extern(volatile, parallel_safe)]
fn drop_custom_model(name: &str) {
    if let Err(e) = validate_new_model_name(&name) {
        pgrx::warning!("Invalid model name: {}, Details: {}", name, e);
        return;
    }

    let select_config = r#"SELECT config FROM tokenizer_catalog.model WHERE name = $1"#;
    let config_bytes: &str = spi_get_one(select_config, &[name.into()]).unwrap();
    let config: ModelConfig = serde_json::from_str(config_bytes).unwrap();
    let ModelConfig::Custom(config) = &config else {
        panic!("Model is not a custom model: {}", name);
    };
    let table_name = quote_identifier(&config.table);

    let drop_model_trigger = format!(
        r#"DROP TRIGGER IF EXISTS "model_{}_trigger" ON {}"#,
        name, table_name
    );
    let drop_insert_trigger = format!(
        r#"DROP TRIGGER IF EXISTS "model_{}_trigger_insert" ON {}"#,
        name, table_name
    );
    let drop_table = format!(r#"DROP TABLE IF EXISTS tokenizer_catalog."model_{}""#, name);
    let delete_model = r#"DELETE FROM tokenizer_catalog.model WHERE name = $1 RETURNING 1"#;

    pgrx::Spi::connect_mut(|client| {
        client.update(&drop_model_trigger, None, &[]).unwrap();
        client.update(&drop_insert_trigger, None, &[]).unwrap();
        client.update(&drop_table, None, &[]).unwrap();
        let tuptable = client.update(delete_model, None, &[name.into()]).unwrap();

        if tuptable.len() == 0 {
            pgrx::warning!("Model not found: {}", name);
        }
    });

    MODEL_OBJECT_POOL.remove(name);
}

const MAX_TOKEN_LENGTH: usize = 2600;

#[pgrx::pg_extern(volatile, parallel_safe)]
fn apply_text_analyzer_for_custom_model(text: &str, analyzer: &str) -> Vec<String> {
    let text_analyzer = get_text_analyzer(analyzer);
    let mut results = text_analyzer.apply(text);

    // split all tokens that are longer than 2600 characters
    let len = results.len();
    for i in 0..len {
        let token_len = results[i].len();
        if token_len > MAX_TOKEN_LENGTH {
            pgrx::warning!("There is a custom table token whose length has exceeded MAX_TOKEN_LENGTH({MAX_TOKEN_LENGTH}). It will be cut off to multiple tokens. If you need to support long token, welcome to submit an issue to \"https://github.com/tensorchord/VectorChord-bm25/issues\".");

            let replace_token = results[i][..MAX_TOKEN_LENGTH].to_string();
            let token = std::mem::replace(&mut results[i], replace_token);
            for j in 1..(token.len().div_ceil(MAX_TOKEN_LENGTH)) {
                results.push(token[j * MAX_TOKEN_LENGTH..][..MAX_TOKEN_LENGTH].to_string());
            }
        }
    }
    results
}

pgrx::extension_sql!(
    r#"
CREATE FUNCTION custom_model_insert_trigger()
RETURNS TRIGGER AS $$
DECLARE
    tokenizer_name TEXT := TG_ARGV[0];
    target_column TEXT := TG_ARGV[1];
    text_analyzer TEXT := TG_ARGV[2];
BEGIN
    EXECUTE format('
    WITH 
    tokens AS (
        SELECT unnest(tokenizer_catalog.apply_text_analyzer_for_custom_model($1.%I, %L)) AS token
    )
    INSERT INTO tokenizer_catalog."model_%s" (token) SELECT token FROM tokens ON CONFLICT (token) DO NOTHING', target_column, text_analyzer, tokenizer_name) USING NEW;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
    "#,
    name = "custom_model_insert_trigger"
);

pgrx::extension_sql!(
    r#"
CREATE FUNCTION create_custom_model_tokenizer_and_trigger(
tokenizer_name TEXT, model_name TEXT, text_analyzer_name TEXT, table_name TEXT, source_column TEXT, target_column TEXT)
RETURNS VOID AS $body$
BEGIN
    EXECUTE format('SELECT tokenizer_catalog.create_custom_model(%L, $$
        table = %L
        column = %L
        text_analyzer = %L
        $$)', model_name, table_name, source_column, text_analyzer_name);
    EXECUTE format('SELECT tokenizer_catalog.create_tokenizer(%L, $$
        text_analyzer = %L
        model = %L
        $$)', tokenizer_name, text_analyzer_name, model_name);
    EXECUTE format('UPDATE %I SET %I = tokenizer_catalog.tokenize(%I, %L)', table_name, target_column, source_column, tokenizer_name);
    EXECUTE format('CREATE TRIGGER "model_%s_trigger_insert" BEFORE INSERT OR UPDATE OF %I ON %I FOR EACH ROW EXECUTE FUNCTION tokenizer_catalog.custom_model_tokenizer_set_target_column_trigger(%L, %I, %I)', model_name, source_column, table_name, tokenizer_name, source_column, target_column);
END;
$body$ LANGUAGE plpgsql;
    "#,
    name = "create_custom_model_tokenizer_and_trigger"
);

#[pgrx::pg_trigger]
fn custom_model_tokenizer_set_target_column_trigger<'a>(
    trigger: &'a pgrx::PgTrigger<'a>,
) -> Result<Option<PgHeapTuple<'a, impl WhoAllocated>>, ()> {
    let mut new = trigger.new().expect("new tuple is missing").into_owned();
    let tg_argv = trigger.extra_args().expect("trigger arguments are missing");
    if tg_argv.len() != 3 {
        panic!("Invalid trigger arguments");
    }
    let tokenizer_name = &tg_argv[0];
    let source_column = &tg_argv[1];
    let target_column = &tg_argv[2];

    let source = new
        .get_by_name::<&str>(source_column)
        .expect("source column is missing");
    let Some(source) = source else {
        return Ok(Some(new));
    };

    let target = crate::tokenizer::tokenize(source, tokenizer_name);
    new.set_by_name(target_column, target)
        .expect("set target column failed");
    Ok(Some(new))
}

fn quote_identifier(ident: &str) -> String {
    unsafe {
        let ptr = pgrx::pg_sys::quote_identifier(ident.as_pg_cstr());
        let quoted_str = CStr::from_ptr(ptr).to_str().unwrap().to_string();
        pgrx::pg_sys::pfree(ptr as _);
        quoted_str
    }
}
